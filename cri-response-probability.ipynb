{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63e7ed41-a3d4-4d29-b4f7-42ea4593a007",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import torch\n",
    "from pathlib import Path\n",
    "import srsly\n",
    "import random\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "transformers.utils.logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b20315b3-8a78-4120-8499-5b3fe4f23f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = list(srsly.read_jsonl(\"testing-data/cri.jsonl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d10b44f-bfdd-45bb-8faa-d892d9f4da3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd12a1e9110a4131ad8af56375293074",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = transformers.AutoModelForCausalLM.from_pretrained('meta-llama/Meta-Llama-3-8B-Instruct').to('cuda')\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained('meta-llama/Meta-Llama-3-8B-Instruct', model_max_length=1024)\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6d4d0202-f064-4606-810e-b95a18bd5f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(chunk, question, answer):\n",
    "    prompt = (\n",
    "        \"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\"\n",
    "        \"\\nYou are an excellent student who has just read the following excerpt.\"\n",
    "        \" The teacher will ask you a question. You will answer accurately.\"\n",
    "        f\"\\n\\nExcerpt:\\n\\n{chunk}<|eot_id|>\"\n",
    "        \"\\n<|start_header_id|>user<|end_header_id|>\"\n",
    "        f\"{question}<|eot_id|>\"\n",
    "        \"\\n<|start_header_id|>assistant<|end_header_id|>\"\n",
    "    )\n",
    "\n",
    "    # Inputs\n",
    "    # Construct inputs separately, so we know where the prompt ends.\n",
    "    prompt_encoding = tokenizer(prompt, return_tensors=\"pt\", return_length=True)\n",
    "    answer_encoding = tokenizer(answer, return_tensors=\"pt\")\n",
    "    input_ids = torch.cat((prompt_encoding.input_ids, answer_encoding.input_ids), 1).to('cuda')\n",
    "\n",
    "    # Targets\n",
    "    target_ids = input_ids.clone()\n",
    "    prompt_length = prompt_encoding.length.item()\n",
    "    # Setting targets to -100 will ignore them when calculating loss\n",
    "    # Do this for all tokens before the answer.\n",
    "    target_ids[:, :prompt_length] = -100\n",
    "\n",
    "    return input_ids, target_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "879e81df-9ad1-4ed3-8dba-9c3b4e0d1d75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86461be2991b4436b19b65f582e1b1ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2540 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def get_loss(input_ids, target_ids):\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, labels=target_ids)\n",
    "\n",
    "    # loss is calculated using CrossEntropyLoss which averages over valid labels\n",
    "    return outputs.loss\n",
    "\n",
    "outputs = []\n",
    "for cri_input in tqdm(inputs):\n",
    "    input_ids, target_ids = tokenize(cri_input[\"Chunk\"], cri_input[\"Question\"], cri_input[\"answer\"])\n",
    "    loss = get_loss(input_ids, target_ids)\n",
    "    # print(f'Score: {cri_input[\"score\"]} | Loss: {loss.item()}')\n",
    "    cri_input[\"Loss\"] = loss.item()\n",
    "    outputs.append(cri_input)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:hf]",
   "language": "python",
   "name": "conda-env-hf-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
